{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "mount_file_id": "1bBN4lLR7KsjaErcbe6ftBl7wBGRr0rnX",
   "authorship_tag": "ABX9TyNOfIpvHgfgDhXAYOiA0lzJ"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Detection of inconsistencies in food descriptions in online food ordering and delivery platform serves as an important ingredient for success, customer retention and satisfaction. Most companies providing online food ordering and delivery platforms are gradually utilising deep learning based solutions to detect if a food image shown on their platform conforms to the description given or category."
   ],
   "metadata": {
    "id": "d6vKcjPHGCXy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ],
   "metadata": {
    "id": "zeEKHU-FJr3Z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1700225181667,
     "user_tz": -180,
     "elapsed": 15554,
     "user": {
      "displayName": "Дмитрий Камышников",
      "userId": "03871553027115189109"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:00.562574400Z",
     "start_time": "2023-11-17T14:54:00.538235300Z"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section we will implement a food classification convolutional neural network which will be trained on food categories Benchmark dataset (Dataset description [1], [2]). The CNN architecture shown in Table 1 is a baseline model. In this section we will need to improve the model performance using minimum of three techniques presented in the course (i.e Dropout, Batch Normalization, etc.) except for transfer learning. The baseline model is to be trained on 10 epochs, using SGD optimizer, learning rate 0.001 and preprocessing of only min-max scaling. The model performance should at each training epoch should be logged to Tensorboard. The performance metrics to be monitored are training and validation loss, accuracy and F1 score. Train and test dataset can be downloaded from **torchvision.datasets.Food101.**"
   ],
   "metadata": {
    "id": "r22J96sFGGoH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "| Layer | Layer Type | Kernel size | Stride | Padding | Out channels |\n",
    "|-------|------------|-------------|--------|---------|--------------|\n",
    "| 0     | Input      | -           | -      | -       | 3            |\n",
    "| 1     | Convolutional | 3 x 3     | 1      | 1 x 1   | 10           |\n",
    "| 2     | Convolutional | 3 x 3     | 1      | 1 x 1   | 10           |\n",
    "| 3     | Max-pooling | 2 x 2       | 2      | -       | -            |\n",
    "| 5     | Convolutional | 3 x 3     | 1      | 1 x 1   | 10           |\n",
    "| 6     | Convolutional | 3 x 3     | 1      | 1 x 1   | 10           |\n",
    "| 7     | Max-pooling | 2 x 2       | 2      | -       | -            |\n",
    "| 8     | Flatten    | -           | -      | -       | -            |\n",
    "| 9     | Fully connected | -       | -      | -       | 2560 (output)|\n",
    "| 10    | Fully connected (softmax) | - | - | -       | 101 (output) |\n"
   ],
   "metadata": {
    "id": "vjyv_045ICgX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Step 2: Load Food101 Dataset\n",
    "Load the Food101 dataset, resize images to 128x128 pixels, convert them to tensors, and normalize pixel values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the full train and test datasets\n",
    "full_train_dataset = datasets.Food101(root='./data', split='train', download=True, transform=transform)\n",
    "full_test_dataset = datasets.Food101(root='./data', split='test', download=True, transform=transform)\n",
    "\n",
    "# Define the size of the subset (e.g., 10,000 samples)\n",
    "subset_size_train = 8000\n",
    "subset_size_test = 2000\n",
    "\n",
    "# Randomly select subset indices\n",
    "subset_indices_train = random.sample(range(len(full_train_dataset)), subset_size_train)\n",
    "subset_indices_test = random.sample(range(len(full_test_dataset)), subset_size_test)\n",
    "\n",
    "# Create a Subset of the train dataset using the selected indices\n",
    "subset_train_dataset = Subset(full_train_dataset, subset_indices_train)\n",
    "\n",
    "# Create a Subset of the train dataset using the selected indices\n",
    "subset_test_dataset = Subset(full_test_dataset, subset_indices_test)\n",
    "\n",
    "# Use DataLoader for the subset train dataset\n",
    "subset_train_loader = DataLoader(subset_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Use DataLoader for the full test dataset\n",
    "subset_test_loader = DataLoader(subset_test_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "id": "jpFUFO18o4ee",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6937421d-3592-4b5a-b78e-d2da14563129",
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.256679100Z",
     "start_time": "2023-11-17T14:54:00.546319700Z"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### FoodCNN Model\n",
    "Define a convolutional neural network (CNN) model named `FoodCNN` for image classification.\n",
    "\n",
    "A CNN is suitable for image classification tasks as it can automatically learn hierarchical features from images. This model consists of convolutional layers to capture spatial features, max-pooling layers to downsample the spatial dimensions, and fully connected layers for classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class FoodCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FoodCNN, self).__init__()\n",
    "\n",
    "        # Layer 1: Convolutional\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        # Layer 2: Convolutional\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        # Layer 3: Max-pooling\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 5: Convolutional\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        # Layer 6: Convolutional\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        # Layer 7: Max-pooling\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 8: Flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Layer 9: Fully connected\n",
    "        self.fc1 = nn.Linear(10240, 2560)\n",
    "        self.relu5 = nn.ReLU()\n",
    "\n",
    "        # Layer 10: Fully connected (softmax)\n",
    "        self.fc2 = nn.Linear(2560, 101)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.relu3(self.conv3(x))\n",
    "        x = self.relu4(self.conv4(x))\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model = FoodCNN()"
   ],
   "metadata": {
    "id": "E_sdMFZ9J-GS",
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.360056300Z",
     "start_time": "2023-11-17T14:54:01.240600400Z"
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### FoodCNN2 Model\n",
    "Define an improved convolutional neural network (CNN) model named `FoodCNN2` for image classification. This model includes batch normalization and dropout layers to enhance training stability and prevent overfitting.\n",
    "\n",
    "Batch normalization normalizes the input of each layer, making the training process more stable and accelerating convergence. Dropout is used during training to randomly \"drop out\" units, preventing co-adaptation of hidden units and reducing overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FoodCNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FoodCNN2, self).__init__()\n",
    "\n",
    "        # Layer 1: Convolutional\n",
    "        self.conv1 = nn.Conv2d(3, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm2d(10)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 2: Convolutional\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.batch_norm2 = nn.BatchNorm2d(10)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 3: Max-pooling\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 5: Convolutional\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.batch_norm3 = nn.BatchNorm2d(10)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 6: Convolutional\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.batch_norm4 = nn.BatchNorm2d(10)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 7: Max-pooling\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 8: Flatten\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Layer 9: Fully connected\n",
    "        self.fc1 = nn.Linear(10240, 2560)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.batch_norm5 = nn.BatchNorm1d(2560)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "\n",
    "        # Layer 10: Fully connected (softmax)\n",
    "        self.fc2 = nn.Linear(2560, 101)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.batch_norm1(self.relu1(self.conv1(x))))\n",
    "        x = self.dropout2(self.batch_norm2(self.relu2(self.conv2(x))))\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.dropout3(self.batch_norm3(self.relu3(self.conv3(x))))\n",
    "        x = self.dropout4(self.batch_norm4(self.relu4(self.conv4(x))))\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout5(self.batch_norm5(self.relu5(self.fc1(x))))\n",
    "        x = self.softmax(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "model2 = FoodCNN2()"
   ],
   "metadata": {
    "id": "5q88uBhGJ_oK",
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.483522700Z",
     "start_time": "2023-11-17T14:54:01.360056300Z"
    }
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### FoodCNN3 Model\n",
    "Define an enhanced convolutional neural network (CNN) model named `FoodCNN3` for image classification. This model incorporates deeper layers, global average pooling, and increased units in fully connected layers for improved feature extraction and discrimination.\n",
    "\n",
    "Deeper layers allow the model to learn more complex hierarchical features. Global average pooling reduces the spatial dimensions before the fully connected layers, capturing the most important features and reducing the risk of overfitting."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class FoodCNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FoodCNN3, self).__init__()\n",
    "\n",
    "        # Layer 1: Convolutional\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 2: Convolutional\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 3: Max-pooling\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 4: Convolutional\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 5: Convolutional\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.batch_norm4 = nn.BatchNorm2d(128)\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "\n",
    "        # Layer 6: Max-pooling\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Layer 7: Global Average Pooling\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Layer 8: Fully connected\n",
    "        self.fc1 = nn.Linear(128, 256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.batch_norm5 = nn.BatchNorm1d(256)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "\n",
    "        # Layer 9: Fully connected (softmax)\n",
    "        self.fc2 = nn.Linear(256, 101)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.batch_norm1(self.relu1(self.conv1(x))))\n",
    "        x = self.dropout2(self.batch_norm2(self.relu2(self.conv2(x))))\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.dropout3(self.batch_norm3(self.relu3(self.conv3(x))))\n",
    "        x = self.dropout4(self.batch_norm4(self.relu4(self.conv4(x))))\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout5(self.batch_norm5(self.relu5(self.fc1(x))))\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Create an instance of the improved model\n",
    "model3 = FoodCNN3()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.494431300Z",
     "start_time": "2023-11-17T14:54:01.488542Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define the loss function and optimizer for training the model.\n",
    "- **Loss Function (`nn.CrossEntropyLoss`):** CrossEntropyLoss is commonly used for multi-class classification problems, which is the case for food image classification. It computes the negative log likelihood of the predicted probability distribution over classes.\n",
    "  \n",
    "- **Optimizer (`optim.SGD`):** Stochastic Gradient Descent (SGD) is chosen as the optimizer. It's a popular optimization algorithm that updates the model parameters to minimize the loss. The learning rate (`lr`) is set to 0.001, which is a common starting value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "id": "1Rx8SWMzKBES",
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.505275600Z",
     "start_time": "2023-11-17T14:54:01.496938600Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define a training function to train the models.\n",
    "\n",
    "- The `train_model` function is responsible for training the model for one epoch. It iterates over the training dataset, computes the loss, performs backpropagation, and updates the model parameters.\n",
    "- The training progress is logged, including loss, accuracy, and F1 score, and the model is saved after each epoch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, writer, epoch):\n",
    "    model.train()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_loss = 0.0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted_labels = torch.argmax(outputs, dim=1)\n",
    "        all_loss += loss.item()\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    average_loss = all_loss / len(train_loader)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    writer.add_scalar(f'Train {model.__class__.__name__}/Loss', average_loss, epoch)\n",
    "    writer.add_scalar(f'Train {model.__class__.__name__}/Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar(f'Train {model.__class__.__name__}/F1_Score', f1, epoch)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{10}, Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}, Train F1 Score: {f1:.4f}')\n",
    "    torch.save(model.state_dict(), f'{model.__class__.__name__}.pth')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.514570100Z",
     "start_time": "2023-11-17T14:54:01.498724700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define an evaluation function to assess the performance of the models on the test set.\n",
    "\n",
    "- The `evaluate_model` function is used to evaluate the model on the test set after training. It calculates the loss, accuracy, and F1 score.\n",
    "- The evaluation results are logged, providing insights into how well the model generalizes to unseen data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion, writer, epoch):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            loss = criterion(outputs, labels)\n",
    "            all_loss += loss.item()\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    average_loss = all_loss / len(test_loader)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    writer.add_scalar(f'Test {model.__class__.__name__}/Loss', average_loss, epoch)\n",
    "    writer.add_scalar(f'Test {model.__class__.__name__}/Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar(f'Test {model.__class__.__name__}/F1_Score', f1, epoch)\n",
    "\n",
    "    print(f'Loss: {average_loss:.4f}, Accuracy: {accuracy:.4f}, Test F1 Score: {f1:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.514570100Z",
     "start_time": "2023-11-17T14:54:01.505275600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TensorBoard Setup\n",
    "Initialize TensorBoard for monitoring the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='task_2.1_Model_FoodCNN1')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T14:54:01.514570100Z",
     "start_time": "2023-11-17T14:54:01.509313800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the main training loop for the `FoodCNN` model on the subset of the Food101 dataset.\n",
    "\n",
    "- The main loop iteratively trains the model on the training set (`subset_train_loader`) and evaluates its performance on the test set (`subset_test_loader`) for a specified number of epochs.\n",
    "- Training and evaluation metrics are logged using Tensorboard (`writer`), providing insights into the model's performance over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.6151, Accuracy: 0.0110, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0003\n",
      "Epoch 2/10, Loss: 4.6151, Accuracy: 0.0110, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0003\n",
      "Epoch 3/10, Loss: 4.6151, Accuracy: 0.0110, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0003\n",
      "Epoch 4/10, Loss: 4.6151, Accuracy: 0.0110, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0003\n",
      "Epoch 5/10, Loss: 4.6151, Accuracy: 0.0110, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0003\n",
      "Epoch 6/10, Loss: 4.6151, Accuracy: 0.0110, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0004\n",
      "Epoch 7/10, Loss: 4.6151, Accuracy: 0.0110, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0004\n",
      "Epoch 8/10, Loss: 4.6151, Accuracy: 0.0109, Train F1 Score: 0.0003\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0004\n",
      "Epoch 9/10, Loss: 4.6151, Accuracy: 0.0109, Train F1 Score: 0.0002\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0004\n",
      "Epoch 10/10, Loss: 4.6151, Accuracy: 0.0109, Train F1 Score: 0.0002\n",
      "Loss: 4.6151, Accuracy: 0.0070, Test F1 Score: 0.0004\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model, subset_train_loader, criterion, optimizer, writer, epoch)\n",
    "    evaluate_model(model, subset_test_loader, criterion, writer, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:13:39.015949300Z",
     "start_time": "2023-11-17T14:54:01.514570100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Close Tensorboard writer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:13:39.018295300Z",
     "start_time": "2023-11-17T15:13:39.015949300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:13:39.028263400Z",
     "start_time": "2023-11-17T15:13:39.018295300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TensorBoard Setup\n",
    "Initialize TensorBoard for monitoring the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='task_2.1_Model_FoodCNN2')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the main training loop for the `FoodCNN2` model on the subset of the Food101 dataset.\n",
    "\n",
    "- The main loop iteratively trains the model on the training set (`subset_train_loader`) and evaluates its performance on the test set (`subset_test_loader`) for a specified number of epochs.\n",
    "- Training and evaluation metrics are logged using Tensorboard (`writer`), providing insights into the model's performance over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.6151, Accuracy: 0.0111, Train F1 Score: 0.0112\n",
      "Loss: 4.6150, Accuracy: 0.0100, Test F1 Score: 0.0080\n",
      "Epoch 2/10, Loss: 4.6151, Accuracy: 0.0101, Train F1 Score: 0.0100\n",
      "Loss: 4.6151, Accuracy: 0.0120, Test F1 Score: 0.0090\n",
      "Epoch 3/10, Loss: 4.6151, Accuracy: 0.0103, Train F1 Score: 0.0101\n",
      "Loss: 4.6151, Accuracy: 0.0115, Test F1 Score: 0.0084\n",
      "Epoch 4/10, Loss: 4.6152, Accuracy: 0.0092, Train F1 Score: 0.0093\n",
      "Loss: 4.6151, Accuracy: 0.0110, Test F1 Score: 0.0084\n",
      "Epoch 5/10, Loss: 4.6151, Accuracy: 0.0111, Train F1 Score: 0.0110\n",
      "Loss: 4.6151, Accuracy: 0.0135, Test F1 Score: 0.0100\n",
      "Epoch 6/10, Loss: 4.6154, Accuracy: 0.0096, Train F1 Score: 0.0097\n",
      "Loss: 4.6151, Accuracy: 0.0105, Test F1 Score: 0.0078\n",
      "Epoch 7/10, Loss: 4.6153, Accuracy: 0.0095, Train F1 Score: 0.0095\n",
      "Loss: 4.6151, Accuracy: 0.0110, Test F1 Score: 0.0096\n",
      "Epoch 8/10, Loss: 4.6152, Accuracy: 0.0092, Train F1 Score: 0.0092\n",
      "Loss: 4.6151, Accuracy: 0.0120, Test F1 Score: 0.0091\n",
      "Epoch 9/10, Loss: 4.6153, Accuracy: 0.0077, Train F1 Score: 0.0076\n",
      "Loss: 4.6150, Accuracy: 0.0110, Test F1 Score: 0.0087\n",
      "Epoch 10/10, Loss: 4.6151, Accuracy: 0.0101, Train F1 Score: 0.0103\n",
      "Loss: 4.6151, Accuracy: 0.0115, Test F1 Score: 0.0086\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model2, subset_train_loader, criterion, optimizer, writer, epoch)\n",
    "    evaluate_model(model2, subset_test_loader, criterion, writer, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:32:55.038629500Z",
     "start_time": "2023-11-17T15:13:39.024268400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Close Tensorboard writer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:32:55.054569400Z",
     "start_time": "2023-11-17T15:32:55.033659700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the TensorBoard notebook extension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T15:32:55.059567400Z",
     "start_time": "2023-11-17T15:32:55.042469600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TensorBoard Setup\n",
    "Initialize TensorBoard for monitoring the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='task_2.1_Model_FoodCNN3')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the main training loop for the `FoodCNN3` model on the subset of the Food101 dataset.\n",
    "\n",
    "- The main loop iteratively trains the model on the training set (`subset_train_loader`) and evaluates its performance on the test set (`subset_test_loader`) for a specified number of epochs.\n",
    "- Training and evaluation metrics are logged using Tensorboard (`writer`), providing insights into the model's performance over time."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 4.8710, Accuracy: 0.0092, Train F1 Score: 0.0072\n",
      "Loss: 13.0147, Accuracy: 0.0100, Test F1 Score: 0.0060\n",
      "Epoch 2/10, Loss: 4.8756, Accuracy: 0.0095, Train F1 Score: 0.0085\n",
      "Loss: 13.2816, Accuracy: 0.0095, Test F1 Score: 0.0048\n",
      "Epoch 3/10, Loss: 4.8786, Accuracy: 0.0100, Train F1 Score: 0.0087\n",
      "Loss: 13.1440, Accuracy: 0.0095, Test F1 Score: 0.0056\n",
      "Epoch 4/10, Loss: 4.8707, Accuracy: 0.0075, Train F1 Score: 0.0069\n",
      "Loss: 13.4031, Accuracy: 0.0080, Test F1 Score: 0.0049\n",
      "Epoch 5/10, Loss: 4.8703, Accuracy: 0.0103, Train F1 Score: 0.0094\n",
      "Loss: 12.8771, Accuracy: 0.0095, Test F1 Score: 0.0056\n",
      "Epoch 6/10, Loss: 4.8911, Accuracy: 0.0103, Train F1 Score: 0.0097\n",
      "Loss: 13.1525, Accuracy: 0.0090, Test F1 Score: 0.0059\n",
      "Epoch 7/10, Loss: 4.8713, Accuracy: 0.0115, Train F1 Score: 0.0096\n",
      "Loss: 12.8194, Accuracy: 0.0080, Test F1 Score: 0.0048\n",
      "Epoch 8/10, Loss: 4.8782, Accuracy: 0.0104, Train F1 Score: 0.0095\n",
      "Loss: 12.6121, Accuracy: 0.0080, Test F1 Score: 0.0046\n",
      "Epoch 9/10, Loss: 4.8691, Accuracy: 0.0105, Train F1 Score: 0.0092\n",
      "Loss: 13.0667, Accuracy: 0.0090, Test F1 Score: 0.0058\n",
      "Epoch 10/10, Loss: 4.8713, Accuracy: 0.0096, Train F1 Score: 0.0085\n",
      "Loss: 13.1380, Accuracy: 0.0110, Test F1 Score: 0.0057\n"
     ]
    }
   ],
   "source": [
    "# Main loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model3, subset_train_loader, criterion, optimizer, writer, epoch)\n",
    "    evaluate_model(model3, subset_test_loader, criterion, writer, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:51:51.207382300Z",
     "start_time": "2023-11-17T15:32:55.044496500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Close Tensorboard writer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:51:51.248472200Z",
     "start_time": "2023-11-17T16:51:51.216260Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the TensorBoard notebook extension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "id": "XShSvYIulnS9",
    "ExecuteTime": {
     "end_time": "2023-11-17T16:51:52.864147Z",
     "start_time": "2023-11-17T16:51:51.246472400Z"
    }
   },
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "For this section, we will need to use transfer learning. You will have to achieve better performance than the improved baseline model in before section. Suggested pretrained model is ‘DENSENET121‘ which can be loaded from torchvision.models.densenet121, however you are not only limited to ‘DENSENET121‘. The model training progress should also be monitored using using tensorboard. The performance metrics to be monitored are training and validation loss, accuracy and F1 score."
   ],
   "metadata": {
    "id": "7LX-zJnCGJ3q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:51:52.909603Z",
     "start_time": "2023-11-17T16:51:52.869152700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Set device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define a set of data transforms using the `transforms.Compose` module from PyTorch.\n",
    "\n",
    "- Data transforms are essential for preparing the input data for the neural network.\n",
    "- In this case, the transforms include resizing the images to (224, 224) pixels, converting them to tensors, and normalizing the pixel values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# Data transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:51:52.939593Z",
     "start_time": "2023-11-17T16:51:52.894727400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the Food101 dataset, create subsets, and use DataLoader for training and testing.\n",
    "\n",
    "- Loading the dataset is a crucial step in preparing data for training and evaluation.\n",
    "- Creating subsets allows you to work with a smaller portion of the dataset, which can be useful for testing or if the full dataset is too large.\n",
    "- DataLoader is employed to efficiently load batches of data during training and testing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "# Load the full train and test datasets\n",
    "full_train_dataset = datasets.Food101(root='./data/food-101', split='train', download=False, transform=transform)\n",
    "full_test_dataset = datasets.Food101(root='./data/food-101', split='test', download=False, transform=transform)\n",
    "\n",
    "# Define the size of the subset (e.g., 10,000 samples)\n",
    "subset_size_train = 8000\n",
    "subset_size_test = 2000\n",
    "\n",
    "# Randomly select subset indices\n",
    "subset_indices_train = random.sample(range(len(full_train_dataset)), subset_size_train)\n",
    "subset_indices_test = random.sample(range(len(full_test_dataset)), subset_size_test)\n",
    "\n",
    "# Create a Subset of the train dataset using the selected indices\n",
    "subset_train_dataset = Subset(full_train_dataset, subset_indices_train)\n",
    "\n",
    "# Create a Subset of the train dataset using the selected indices\n",
    "subset_test_dataset = Subset(full_test_dataset, subset_indices_test)\n",
    "\n",
    "# Use DataLoader for the subset train dataset\n",
    "subset_train_loader = DataLoader(subset_train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Use DataLoader for the full test dataset\n",
    "subset_test_loader = DataLoader(subset_test_dataset, batch_size=32, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:51:54.282236100Z",
     "start_time": "2023-11-17T16:51:52.911595700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define a transfer learning model using DENSENET121 with a custom output layer for the specified number of classes.\n",
    "\n",
    "- Transfer learning allows leveraging pre-trained models on large datasets to boost performance on a smaller dataset.\n",
    "- DENSENET121 is a deep convolutional neural network known for its effectiveness in image classification tasks.\n",
    "- Modifying the fully connected layer is necessary to match the number of classes in your specific classification problem."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "# Define DENSENET121 model with custom output layer\n",
    "class TransferLearningModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(TransferLearningModel, self).__init__()\n",
    "        # Load pre-trained DENSENET121 model\n",
    "        self.base_model = models.densenet121(pretrained=True)\n",
    "        # Freeze convolutional layers\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Modify the fully connected layer for the custom number of classes\n",
    "        self.base_model.classifier = nn.Sequential(\n",
    "            nn.Linear(1024, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:51:54.354055700Z",
     "start_time": "2023-11-17T16:51:54.279837100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create an instance of the transfer learning model and define the loss function and optimizer for training.\n",
    "\n",
    "- Creating an instance of the model is necessary to start the training process.\n",
    "- The number of classes is determined by the length of the classes in the training dataset.\n",
    "- CrossEntropyLoss is commonly used for multi-class classification tasks.\n",
    "- Adam optimizer is chosen for its adaptive learning rate properties."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\progr\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to C:\\Users\\progr/.cache\\torch\\hub\\checkpoints\\densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [06:19<00:00, 85.3kB/s]  \n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model\n",
    "num_classes = len(full_train_dataset.classes)\n",
    "modelTransferLearningModel = TransferLearningModel(num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:59:01.649342400Z",
     "start_time": "2023-11-17T16:51:54.291810100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### TensorBoard Setup\n",
    "Initialize TensorBoard for monitoring the model's performance during training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "writer = SummaryWriter(comment='task_2.2_Model_DENSENET121')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T16:59:01.650413Z",
     "start_time": "2023-11-17T16:59:01.647328900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train the Transfer Learning Model on the subset of the Food101 dataset and evaluate its performance on the validation set. \n",
    "\n",
    "- Training the model involves iterating through the training data, calculating the loss, and updating the model parameters.\n",
    "- Evaluating the model on the validation set helps monitor its performance and prevent overfitting.\n",
    "- Metrics such as accuracy and F1 score provide insights into the model's classification performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 250/250 [07:27<00:00,  1.79s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:18<00:00,  1.25s/it]\n",
      "Epoch 2/10: 100%|██████████| 250/250 [05:56<00:00,  1.43s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:19<00:00,  1.27s/it]\n",
      "Epoch 3/10: 100%|██████████| 250/250 [09:12<00:00,  2.21s/it]\n",
      "Validation: 100%|██████████| 63/63 [02:11<00:00,  2.08s/it]\n",
      "Epoch 4/10: 100%|██████████| 250/250 [10:01<00:00,  2.40s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:30<00:00,  1.43s/it]\n",
      "Epoch 5/10: 100%|██████████| 250/250 [08:08<00:00,  1.95s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:50<00:00,  1.75s/it]\n",
      "Epoch 6/10: 100%|██████████| 250/250 [08:27<00:00,  2.03s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:48<00:00,  1.71s/it]\n",
      "Epoch 7/10: 100%|██████████| 250/250 [07:51<00:00,  1.88s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:41<00:00,  1.62s/it]\n",
      "Epoch 8/10: 100%|██████████| 250/250 [07:56<00:00,  1.90s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:24<00:00,  1.34s/it]\n",
      "Epoch 9/10: 100%|██████████| 250/250 [07:09<00:00,  1.72s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:29<00:00,  1.42s/it]\n",
      "Epoch 10/10: 100%|██████████| 250/250 [06:51<00:00,  1.65s/it]\n",
      "Validation: 100%|██████████| 63/63 [01:45<00:00,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    modelTransferLearningModel.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for inputs, labels in tqdm(subset_train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = modelTransferLearningModel(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Log metrics to Tensorboard\n",
    "    writer.add_scalar('Train/Loss', running_loss / len(subset_train_loader), epoch)\n",
    "    writer.add_scalar('Train/Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('Train/F1_Score', f1, epoch)\n",
    "\n",
    "    # Validation loop\n",
    "    modelTransferLearningModel.eval()\n",
    "    val_loss = 0.0\n",
    "    all_labels_val = []\n",
    "    all_predictions_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs_val, labels_val in tqdm(subset_test_loader, desc=f\"Validation\"):\n",
    "            inputs_val, labels_val = inputs_val.to(device), labels_val.to(device)\n",
    "\n",
    "            outputs_val = modelTransferLearningModel(inputs_val)\n",
    "            loss_val = criterion(outputs_val, labels_val)\n",
    "            val_loss += loss_val.item()\n",
    "\n",
    "            _, predicted_val = torch.max(outputs_val, 1)\n",
    "            all_labels_val.extend(labels_val.cpu().numpy())\n",
    "            all_predictions_val.extend(predicted_val.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics for validation\n",
    "    accuracy_val = accuracy_score(all_labels_val, all_predictions_val)\n",
    "    f1_val = f1_score(all_labels_val, all_predictions_val, average='weighted')\n",
    "\n",
    "    # Log metrics to Tensorboard\n",
    "    writer.add_scalar(f'Validation {modelTransferLearningModel.__class__.__name__}/Loss', val_loss / len(subset_test_loader), epoch)\n",
    "    writer.add_scalar(f'Validation {modelTransferLearningModel.__class__.__name__}/Accuracy', accuracy_val, epoch)\n",
    "    writer.add_scalar(f'Validation {modelTransferLearningModel.__class__.__name__}/F1_Score', f1_val, epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T18:34:23.539170300Z",
     "start_time": "2023-11-17T16:59:01.652432900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Save modelTransferLearningModel"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "torch.save(modelTransferLearningModel.state_dict(), f'{modelTransferLearningModel.__class__.__name__}.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T18:34:23.639563200Z",
     "start_time": "2023-11-17T18:34:23.571815700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Close Tensorboard writer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T18:34:23.571815700Z",
     "start_time": "2023-11-17T18:34:23.554550800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load the TensorBoard notebook extension"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-acc193f071493495\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-acc193f071493495\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T18:34:23.666649100Z",
     "start_time": "2023-11-17T18:34:23.642591600Z"
    }
   }
  }
 ]
}
